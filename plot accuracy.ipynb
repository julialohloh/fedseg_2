{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to get pixel array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import utils\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "def get_pixels_array(filelist: list)-> np.array:\n",
    "    \"\"\"Function to obtain pixels array of image in filelist\n",
    "    \n",
    "    Args:\n",
    "        filelist (list): list of image paths, can be RGB or greyscale\n",
    "    \n",
    "    Returns:\n",
    "        np.array(list_pixels_arr): Array of list of arrays with pixel values\n",
    "        of each image\n",
    "    \"\"\"\n",
    "\n",
    "    list_pixels_arr = []\n",
    "\n",
    "    for img in filelist:\n",
    "        im = (Image.open(img))\n",
    "        transform = transforms.ToTensor()\n",
    "        tensor = transform(im)\n",
    "        list_pixels_arr.append(tensor)\n",
    "\n",
    "    return list_pixels_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get image array tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_paths_1 = utils.get_files_path('train/masks')\n",
    "file_paths_2 = utils.get_files_path('train/masks')\n",
    "\n",
    "list_pixels_arr_1 = get_pixels_array(file_paths_1)\n",
    "list_pixels_arr_2 = get_pixels_array(file_paths_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getaccuracy(list_pixels_arr_1, list_pixels_arr_2):\n",
    "    accuracy_dict = {}\n",
    "    for i in range(len(list_pixels_arr_1)):\n",
    "        y_pred = list_pixels_arr_1[i]\n",
    "        y_true = list_pixels_arr_2[i]\n",
    "        with torch.no_grad():\n",
    "            correct = torch.eq(y_pred, y_true).int()\n",
    "            accuracy = float(correct.sum()) / float(correct.numel())\n",
    "            accuracy_dict[i] = accuracy\n",
    "    return accuracy_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dict = getaccuracy(list_pixels_arr_1, list_pixels_arr_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiDICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiDICE(list_pixels_arr_1, list_pixels_arr_2, num_classes: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Compute multiclass Sorensen Dice coefficient score between predicted and groundtruth\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.tensor): predicted mask from HRNET model\n",
    "        labels (torch.tensor): ground truth mask\n",
    "\n",
    "    Returns:\n",
    "        dice_score (torch.tensor)\n",
    "\n",
    "    \"\"\"\n",
    "    # dice = torch.mean(dice)\n",
    "    for i in range(len(list_pixels_arr_1)):\n",
    "        y_pred = list_pixels_arr_1[i]\n",
    "        y_true = list_pixels_arr_2[i]\n",
    "        # if num_classes <= 0:\n",
    "        #     raise ValueError(\"num_classes must be more than zero\")\n",
    "\n",
    "        # if y_pred.shape != y_true.shape:\n",
    "        #     raise ValueError(\"outputs and labels should be of the same shape\")\n",
    "        dice_dict = {}\n",
    "        dice_score = []\n",
    "        for num in range(0, num_classes):\n",
    "            intersection = ((y_pred == num) * (y_true == num)).sum()\n",
    "            dice_sum = ((y_pred == num).sum() + (y_true == num).sum())\n",
    "            if dice_sum == 0:\n",
    "                dice_score.append(float(\"nan\"))\n",
    "            else:\n",
    "                dice_score.append((2 * intersection)/dice_sum)\n",
    "            dice_dict[i] = torch.nanmean(torch.tensor(dice_score))\n",
    "    return dice_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{9: tensor(1.)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice_score = multiDICE(list_pixels_arr_1, list_pixels_arr_2, 1)\n",
    "dice_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump dict in JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "json_object = json.dumps(accuracy_dict, indent = 4)\n",
    "if os.path.exists(\"json_file\"):\n",
    "    with open('json_file/p1.json', \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "else:\n",
    "    os.makedirs('json_file')\n",
    "    with open('json_file/p1.json', \"w\") as outfile:\n",
    "        outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open json and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('json_file/p1.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    plt.plot(list(data.keys()),list(data.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17fe00490cc2e9db9aee342753cc9fab44bd340278fc543adf814f583e433ba5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
