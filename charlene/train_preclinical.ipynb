{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b2ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import h5py\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.autograd import Function, Variable\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "from networks.data_utils import get_imdb_data, ImdbData, RandomTransforms\n",
    "from networks.relay_net import ReLayNet\n",
    "from networks.net_api.losses import DiceLoss, CrossEntropyLoss2d\n",
    "from solver import Solver, TrainSolver\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6485ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint, params):\n",
    "    \n",
    "    \"\"\"Load checkpoint\n",
    "        Args:\n",
    "            checkpoint (str): location of model checkpoint\n",
    "            params (dict) : dictionary of parameters from train_preclinical.yaml\n",
    "            \n",
    "        Returns:\n",
    "            relaynet_model : model instance\n",
    "\n",
    "    \"\"\"\n",
    "    relaynet_model =  torch.load(checkpoint)\n",
    "    layer_counter = 0\n",
    "    for (name, module) in relaynet_model.named_children():\n",
    "        if 'encode' in name:\n",
    "            for layer in module.children():\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "                print('Layer \"{}\" in module \"{}\" was frozen!'.format(layer_counter, name))\n",
    "                layer_counter+=1\n",
    "    params['num_channels'] = 64\n",
    "    relaynet_model.classifier = nn.Conv2d(params['num_channels'], params['num_class'], params['kernel_c'], params['stride_conv']) \n",
    "    return relaynet_model\n",
    "\n",
    "def train_only(images, labels, wmaps, dimensions, model_path, exp_dir_name, param, checkpoint = None):\n",
    "    \n",
    "    \"\"\"carry out training\n",
    "        Args:\n",
    "            images (numpy array): array of images in training dataset\n",
    "            labels (numpy array): array of labels in training dataset\n",
    "            wmaps (numpy array): array of weighted matrix in training dataset\n",
    "            dimensions (dict): contains height, width and number of layers\n",
    "            model_path (str): file path to save model checkpoints\n",
    "            exp_dir_name (str): name of experiment\n",
    "            param (dict): dictionary of parameters from train_preclinical.yaml\n",
    "            checkpoint (str or None): location of checkpoint, if any \n",
    "            \n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    random_transform = RandomTransforms(dimensions)\n",
    "    train_dataset = ImdbData(config, images, labels, wmaps, dimensions, transform = random_transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "    device = torch.device(\"cuda\")\n",
    "    num_epochs = param['epochs']\n",
    "    \n",
    "    if checkpoint is None:\n",
    "        print('--------Training from scratch--------')\n",
    "        relaynet_model = ReLayNet(param)\n",
    "        \n",
    "    else:\n",
    "        print('--------Loading Checkpoint from pretrained model--------')\n",
    "        relaynet_model = load_checkpoint(checkpoint, param)\n",
    "\n",
    "        \n",
    "    solver = TrainSolver(device, num_class = dimensions['layers'])\n",
    "    solver.train(relaynet_model, train_loader, model_path=model_path, num_epochs=num_epochs, log_nth=1,  exp_dir_name=exp_dir_name)\n",
    "    \n",
    "def train_and_val(train_images, train_labels, train_wmaps, \\\n",
    "                  val_images, val_labels, val_wmaps, \\\n",
    "                  dimensions, model_path, exp_dir_name, param,\n",
    "                 checkpoint = None):\n",
    "    \n",
    "    \"\"\"carry out training & validation\n",
    "        Args:\n",
    "            train_images (numpy array): array of images in training dataset\n",
    "            train_labels (numpy array): array of labels in training dataset\n",
    "            train_wmaps (numpy array): array of weighted matrix in training dataset\n",
    "            val_images (numpy array): array of images in val dataset\n",
    "            val_labels (numpy array): array of labels in val dataset\n",
    "            val_wmaps (numpy array): array of weighted matrix in val dataset\n",
    "            dimensions (dict): contains height, width and number of layers\n",
    "            model_path (str): file path to save model checkpoints\n",
    "            exp_dir_name (str): name of experiment\n",
    "            param (dict): dictionary of parameters from train_preclinical.yaml\n",
    "            checkpoint (str or None): location of checkpoint, if any \n",
    "            \n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    random_transform = RandomTransforms(dimensions)\n",
    "    train_dataset = ImdbData(config, train_images, train_labels, train_wmaps, dimensions, transform = random_transform)\n",
    "    val_dataset = ImdbData(config, val_images, val_labels, val_wmaps, dimensions)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=1)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=1)\n",
    "    device = torch.device(\"cuda\")\n",
    "    num_epochs = param['epochs']\n",
    "    \n",
    "    if checkpoint is None:\n",
    "        print('--------Training from scratch--------')\n",
    "        relaynet_model = ReLayNet(param)\n",
    "    else:\n",
    "        print('--------Loading Checkpoint from pretrained model--------')\n",
    "        relaynet_model = load_checkpoint(checkpoint, param)\n",
    "        \n",
    "    solver = Solver(device, num_class = dimensions['layers'])\n",
    "    solver.train(relaynet_model, train_loader, val_loader, model_path=model_path, num_epochs=num_epochs, log_nth=1,  exp_dir_name=exp_dir_name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4732fd91-f3e9-4e33-a445-be37488a4a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( \"./train_preclinical.yaml\") as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "exp_dir_name = config['filepaths']['exp_dir_name']\n",
    "model_path = config['filepaths']['model_path']\n",
    "data_dir = config['filepaths']['processed_data_path']\n",
    "mode = config['filepaths']['mode']\n",
    "param = config['param']\n",
    "checkpoint = config['checkpoint']['choosen_checkpoint']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e7a8c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Training from scratch--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/SFS/user/kw/ongchar/anaconda3/envs/py37V/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAIN.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/900 [00:00<?, ?it/s]/SFS/user/kw/ongchar/anaconda3/envs/py37V/lib/python3.7/site-packages/torch/nn/functional.py:749: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n",
      "100%|█████████████████████████████████████████| 900/900 [02:47<00:00,  5.39it/s]\n",
      "100%|█████████████████████████████████████████| 300/300 [00:24<00:00, 12.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch : 0 / 6]: average train dice 0.98 \t average val dice 0.99 \t average training loss 0.02\n",
      "Validation loss decreased (inf --> -0.989403).  Saving model ...\n",
      "Saving model... ./models/FL_Partition_Model_2/relaynet_epoch1.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████| 900/900 [02:47<00:00,  5.37it/s]\n",
      "100%|█████████████████████████████████████████| 300/300 [00:24<00:00, 12.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch : 1 / 6]: average train dice 0.99 \t average val dice 0.99 \t average training loss 0.05\n",
      "Validation loss decreased (-0.989403 --> -0.989647).  Saving model ...\n",
      "Saving model... ./models/FL_Partition_Model_2/relaynet_epoch2.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████| 900/900 [02:47<00:00,  5.36it/s]\n",
      "100%|█████████████████████████████████████████| 300/300 [00:24<00:00, 12.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch : 2 / 6]: average train dice 0.99 \t average val dice 0.99 \t average training loss 0.03\n",
      "EarlyStopping counter: 1 out of 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████| 900/900 [02:47<00:00,  5.37it/s]\n",
      "100%|█████████████████████████████████████████| 300/300 [00:24<00:00, 12.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch : 3 / 6]: average train dice 0.99 \t average val dice 0.99 \t average training loss 0.03\n",
      "Validation loss decreased (-0.989647 --> -0.990527).  Saving model ...\n",
      "Saving model... ./models/FL_Partition_Model_2/relaynet_epoch4.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████| 900/900 [02:47<00:00,  5.36it/s]\n",
      "100%|█████████████████████████████████████████| 300/300 [00:25<00:00, 11.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch : 4 / 6]: average train dice 0.99 \t average val dice 0.99 \t average training loss 0.02\n",
      "EarlyStopping counter: 1 out of 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████| 900/900 [02:48<00:00,  5.36it/s]\n",
      "100%|█████████████████████████████████████████| 300/300 [00:24<00:00, 12.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch : 5 / 6]: average train dice 0.99 \t average val dice 0.99 \t average training loss 0.04\n",
      "EarlyStopping counter: 2 out of 2\n",
      "Early stopping\n",
      "FINISH.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if mode == 'default' or mode == 'combine':\n",
    "\n",
    "    train_images, train_labels, train_wmaps, val_images, val_labels, val_wmaps = get_imdb_data(data_dir)\n",
    "\n",
    "    train_images2 = np.copy(np.expand_dims(train_images, axis = 1))\n",
    "    train_labels2 = np.copy(train_labels)\n",
    "    train_wmaps2 = np.copy(train_wmaps)\n",
    "    val_images2 = np.copy(np.expand_dims(val_images, axis = 1))\n",
    "    val_labels2 = np.copy(val_labels)\n",
    "    val_wmaps2 = np.copy(val_wmaps)\n",
    "      \n",
    "    dimensions = {'height': train_labels2.shape[2], 'width':train_labels2.shape[3], 'layers': train_labels2.shape[1]}\n",
    "\n",
    "    if mode == 'combine': # combine training & validation\n",
    "        train_images3 = np.concatenate((train_images2, val_images2), axis=0)\n",
    "        train_labels3 = np.concatenate((train_labels2, val_labels2), axis=0)\n",
    "        train_wmaps3 = np.concatenate((train_wmaps2, val_wmaps2), axis=0)  \n",
    "        train_only(train_images3, train_labels3, train_wmaps3, dimensions, model_path, exp_dir_name, param, checkpoint)\n",
    "\n",
    "    elif mode == 'default':\n",
    "        train_and_val(train_images2, train_labels2, train_wmaps2, \\\n",
    "                  val_images2, val_labels2, val_wmaps2, \\\n",
    "                  dimensions, model_path, exp_dir_name, param, checkpoint)\n",
    "    \n",
    "elif mode == 'train':\n",
    "    with h5py.File(os.path.join(data_dir,'training_intermediate.hdf5'),'r') as hf: \n",
    "        train_images=hf['data'][()]\n",
    "        train_labels=hf['lmap'][()]\n",
    "        train_wmaps=hf['wmap'][()]\n",
    "    train_images2 = np.copy(np.expand_dims(train_images, axis = 1))\n",
    "    train_labels2 = np.copy(train_labels)\n",
    "    train_wmaps2 = np.copy(train_wmaps)\n",
    "    dimensions = {'height': train_labels2.shape[2], 'width':train_labels2.shape[3], 'layers': train_labels2.shape[1]}\n",
    "    train_only(train_images2, train_labels2, train_wmaps2, dimensions, model_path, exp_dir_name, param, checkpoint)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ececee51-8f67-4b65-b873-fca41cbbda66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
