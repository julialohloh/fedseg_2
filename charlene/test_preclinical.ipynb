{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fcc7c71",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e6bd6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import scipy.io\n",
    "import sys\n",
    "import h5py\n",
    "import yaml\n",
    "import sklearn\n",
    "import warnings\n",
    "import tqdm\n",
    "import csv\n",
    "import cv2\n",
    "import argparse\n",
    "from skimage import io\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable \n",
    "torch.manual_seed(0)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aea849a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    import h5py\n",
    "    \n",
    "def dice(x_predict,y_predict):\n",
    "    \n",
    "    \"\"\"calculate dice coefficient\n",
    "        Args:\n",
    "            x_predict (np.bool): ground truth\n",
    "            y_predict (np.bool): predicted\n",
    "            \n",
    "        Returns:\n",
    "            dice coefficient\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if x_predict.shape != y_predict.shape:\n",
    "        raise ValueError('Shape mismatch')\n",
    "\n",
    "    intersection=np.logical_and(x_predict,y_predict)\n",
    "    \n",
    "    return 2.*intersection.sum()/(x_predict.sum()+y_predict.sum())\n",
    "\n",
    "def evaluate(x_predict, y_predict):\n",
    "    \"\"\"obtain the dice coefficient and return it\n",
    "        Args:\n",
    "            x_predict (np.bool): ground truth\n",
    "            y_predict (np.bool): predicted\n",
    "            \n",
    "        Returns:\n",
    "            metrices (numpy array): dice coefficient\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    metrices=dice(x_predict, y_predict)\n",
    "    return metrices\n",
    "\n",
    "def statistics(metric_layer_dict,results_folder, results_name):\n",
    "    \n",
    "    \"\"\"plot box plot of dice coefficient\n",
    "        Args:\n",
    "            metric_layer_dict (dict): dictionary of dice coefficient\n",
    "            results_folder (str): location to save plot\n",
    "            results_name (str): name of plot\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "    \"\"\"\n",
    "    # to exclude background from plots\n",
    "    metric_layer = [l for l in metric_layer_dict.values()]\n",
    "    layer_list = [l for l in metric_layer_dict.keys()]\n",
    "    plt.clf()\n",
    "    fig=plt.figure(1,figsize=(40,15))\n",
    "    ax=fig.add_subplot(111)\n",
    "#     bp=ax.boxplot(metric_layer[:-1]);\n",
    "#     ax.set_xticklabels(layer_list[:-1])\n",
    "    bp=ax.boxplot(metric_layer[1:2]);\n",
    "    ax.set_xticklabels(layer_list[1:2])\n",
    "    ax.set_ylim([0,1])\n",
    "    plt.setp(ax.get_xticklabels(), rotation=15, horizontalalignment='right')\n",
    "    plt.savefig(os.path.join(results_folder,results_name+'.png'))\n",
    "    \n",
    "    \n",
    "class TestData(data.Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.X[index]\n",
    "        img = torch.from_numpy(img)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "def get_predictions(BScans, dimensions, relaynet_model):\n",
    "    \n",
    "    \"\"\"get predictions with pretrained model\n",
    "        Args:\n",
    "            BScans (numpy array): image array\n",
    "            dimensions (dict): contains number of bscans, layers, height and width\n",
    "            relaynet_model : pytorch model\n",
    "            \n",
    "        Returns:\n",
    "            stitched_stauck (numpy array): predicted segmentations\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    stitched_stack = np.zeros((dimensions['bscans'], \n",
    "                                    dimensions['layers'], dimensions['height'], dimensions['width']), dtype=np.float32)\n",
    "    test_dataset = TestData(BScans)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    for idx, (img) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            out = relaynet_model(Variable(img.float().cuda()))\n",
    "        out = F.softmax(out,dim=1)\n",
    "        \n",
    "        stitched_stack[idx] = np.squeeze(out.data.cpu().numpy())\n",
    "        \n",
    "    return stitched_stack\n",
    "\n",
    "def build_mask(annotations,HEIGHT,WIDTH): \n",
    "\n",
    "    \"\"\" Build mask for 1 scan\n",
    "    \"\"\"\n",
    "    layers=np.zeros((annotations.shape[0],HEIGHT,WIDTH), dtype=np.float32)\n",
    "    background=np.ones((HEIGHT,WIDTH))\n",
    "    for layer in range(annotations.shape[0]-1): # loop through the layers\n",
    "        for i in range(annotations.shape[1]): # loop through the width of the image\n",
    "            if int(np.round(annotations[layer, i]))-1>0 and int(np.round(annotations[layer+1, i]))-1>0:\n",
    "            # build channels of mask\n",
    "                layers[layer,np.round(annotations[layer, i]).astype(int)-1:np.round(annotations[layer+1, i]).astype(int)-1,i]=1 # need to -1 to convert matlab index to python index \n",
    "    # get background\n",
    "    layers[-1]=background-np.sum(layers[:-1,:,:],0)\n",
    "    return layers\n",
    "\n",
    "def convert_label_png_to_mask(annotation, HEIGHT, WIDTH):\n",
    "    ''' Converts 2 dimentional png label input into one-hot encoded target mask '''\n",
    "    layers = np.zeros((3,HEIGHT,WIDTH)) #hardcoded for this instance, can change to max of annotation\n",
    "    for h in range(HEIGHT):\n",
    "        for w in range(WIDTH):\n",
    "            layers[annotation[h,w], h,w] = 1\n",
    "            \n",
    "    return layers\n",
    "\n",
    "def prepare_dataset(image_path, label_path=None):\n",
    "\n",
    "    image_stack = io.imread(image_path)\n",
    "    if image_stack.dtype == 'uint8':\n",
    "        image_stack = image_stack/255\n",
    "    elif image_stack.dtype == 'uint16':\n",
    "        image_stack = image_stack*(1.0 / 65535.0)\n",
    "#         image_stack = (image_stack-np.min(image_stack, axis=(1,2))[:,None,None])/(np.max(image_stack, axis=(1,2))[:,None,None]-np.min(image_stack, axis=(1,2))[:,None,None])\n",
    "        \n",
    "    # load corresponding label\n",
    "    images_array = image_stack     \n",
    "    \n",
    "    if label_path is not None:\n",
    "        mat = scipy.io.loadmat(label_path)\n",
    "        annotations=mat['surface_matrix'] \n",
    "        undefine_surface =mat['undefine_surface']\n",
    "        annotations = np.multiply(annotations, undefine_surface)\n",
    "\n",
    "        lmap_array = []\n",
    "\n",
    "        for scan in range(images_array.shape[0]):\n",
    "            image = images_array[scan]\n",
    "            lmap = build_mask(annotations[:,:,scan],image.shape[0],image.shape[1])\n",
    "            lmap_array.append(lmap)\n",
    "\n",
    "        lmap_array = np.array(lmap_array)\n",
    "        return images_array, lmap_array\n",
    "    else:\n",
    "        return images_array\n",
    "    \n",
    "def get_coordinates_for_crop(mip, height, y_top):\n",
    "    ret, thresh1 = cv2.threshold(np.uint8(mip), 10, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = np.ones((10,10),np.uint8)\n",
    "    thresh_opening = cv2.morphologyEx(thresh1, cv2.MORPH_OPEN, kernel)\n",
    "    # stat matrix contains information about bounding box\n",
    "    (num_labels ,labelled_matrix, stat_matrix, centroid_matrix) = cv2.connectedComponentsWithStats(np.uint8(thresh_opening)) \n",
    "    \n",
    "    \n",
    "    max_area = stat_matrix\n",
    "    all_idx = []\n",
    "    for stat in range(stat_matrix.shape[0]):\n",
    "        left_coordinate = stat_matrix[stat,0]\n",
    "        top_coordinate = stat_matrix[stat,1]\n",
    "        bottom_coordinate = stat_matrix[stat,1] - stat_matrix[stat,3]\n",
    "        right_coordinate = stat_matrix[stat,0] + stat_matrix[stat,2]\n",
    "        area = stat_matrix[stat,4]\n",
    "        if left_coordinate == 0 or right_coordinate == 0 or top_coordinate == 0 or bottom_coordinate == 0:\n",
    "            labelled_matrix[labelled_matrix == stat] = 0\n",
    "\n",
    "    y_top_intervals = range(y_top, labelled_matrix.shape[0], 20)\n",
    "    \n",
    "    all_area = []\n",
    "    for y_interval in y_top_intervals:\n",
    "        area = np.count_nonzero(labelled_matrix[y_interval:y_interval+height])/(height*labelled_matrix.shape[1])\n",
    "        all_area.append(area)\n",
    "    idx = np.argmax(np.asarray(all_area))\n",
    "    y_top_idx = y_top_intervals[idx]\n",
    "    return y_top_idx, ret\n",
    "\n",
    "def filter_image_quality(IM, save_imquality, dataset, threshold = 0.01, height_ratio = 0.25):\n",
    "    y_top = 0\n",
    "    height = round(IM.shape[2]*height_ratio)\n",
    "    IM_MAX = np.max(IM, axis=0)\n",
    "\n",
    "    y_top_idx, ret = get_coordinates_for_crop(IM_MAX[0]*255, height, y_top)\n",
    "    ratio = np.squeeze(np.count_nonzero(IM[:,:,y_top_idx:y_top_idx+height]>ret/255, axis=(2,3))/(height*IM.shape[3]))\n",
    "    scans_idx = list(range(0, IM.shape[0]))\n",
    "\n",
    "    filtered_scans = [[scan, r] for scan, r in zip(scans_idx, ratio) if r > threshold]\n",
    "    df = pd.DataFrame(filtered_scans, columns= ['scan', 'ratio'])\n",
    "    df.to_csv(os.path.join(save_imquality, dataset +'.csv'), index=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5c5fd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/87 [00:00<?, ?it/s]/SFS/user/kw/ongchar/anaconda3/envs/py37V/lib/python3.7/site-packages/torch/nn/functional.py:749: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n",
      "  3%|#5                                          | 3/87 [00:45<23:04, 16.49s/it]"
     ]
    }
   ],
   "source": [
    "with open( \"./test_preclinical_evaluate.yaml\") as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    filepaths = config['filepaths']\n",
    "    save_filepaths = config['save_filepaths']\n",
    "    layer_mapping = config['general']['layers_mapping']\n",
    "    test_dataset_list = filepaths['test_dataset']\n",
    "    get_image_quality = config['general']['get_image_quality']\n",
    "    relaynet_model =  torch.load(filepaths['model_path'])\n",
    "    relaynet_model.eval()\n",
    "\n",
    "    if test_dataset_list is not None:\n",
    "        all_datasets = []\n",
    "        with open(test_dataset_list,'r') as reader:\n",
    "            for idx, line in enumerate(reader.readlines()):\n",
    "                all_datasets.append(line.strip('\\n'))\n",
    "    else:\n",
    "        all_datasets = [os.path.basename(f) for f in glob.glob(os.path.join(filepaths['image_path'], '*')) if os.path.isdir(f)]\n",
    "\n",
    "\n",
    "    all_predictions = []\n",
    "    metric_layer_dict={value:[] for value in layer_mapping.values()}\n",
    "    counter=0\n",
    "    for dataset in tqdm.tqdm(all_datasets, ascii=True):  \n",
    "        save_pred = os.path.join(save_filepaths['predictions_path'], 'labels', dataset)\n",
    "        save_eval = os.path.join(save_filepaths['evaluations_path'], dataset)\n",
    "        save_pred_viz = os.path.join(save_filepaths['predictions_path'], 'viz_labels', dataset)\n",
    "        save_imquality = os.path.join(save_filepaths['image_quality_path'])\n",
    "        save_npy = os.path.join(save_filepaths['predictions_path'], os.path.dirname(dataset))\n",
    "\n",
    "        Path(save_pred).mkdir(parents=True, exist_ok = True)\n",
    "        Path(save_eval).mkdir(parents=True, exist_ok = True)\n",
    "        Path(save_npy).mkdir(parents=True, exist_ok = True)\n",
    "        Path(save_pred_viz).mkdir(parents=True, exist_ok = True)\n",
    "\n",
    "        if get_image_quality: Path(save_imquality).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        image_paths = glob.glob(os.path.join(filepaths['image_path'], dataset, '*'+filepaths['image_ext']))\n",
    "        image_paths = sorted(image_paths, key = lambda x:int(os.path.splitext(os.path.basename(x))[0].split('_')[-1]))\n",
    "\n",
    "        images_array = []\n",
    "        lmap_array = []\n",
    "        \n",
    "        for i in range(len(image_paths)):\n",
    "            image_path = image_paths[i]\n",
    "            img = io.imread(image_path)\n",
    "            if img.dtype == 'uint8':\n",
    "                img = img/255\n",
    "            elif img.dtype == 'uint16':\n",
    "                img = img*(1.0 / 65535.0)\n",
    "            \n",
    "            #Find corresponding label \n",
    "            image_path_stem = Path(image_paths[i]).stem\n",
    "            label_path = label_fullpath = os.path.join(filepaths['label_path'],dataset,image_path_stem+'.png')\n",
    "            label = io.imread(label_path)\n",
    "            lmap = convert_label_png_to_mask(label,label.shape[0], label.shape[1])\n",
    "            \n",
    "            #Appending image and lmap to respective arrays\n",
    "            images_array.append(img)\n",
    "            lmap_array.append(lmap)\n",
    "            \n",
    "        images_array = np.array(images_array, dtype=np.float32)\n",
    "        lmap_array = np.array(lmap_array)\n",
    "        images_array2 = np.expand_dims(images_array, axis = 1)\n",
    "\n",
    "        if get_image_quality: df = filter_image_quality(images_array2, save_imquality, dataset)\n",
    "\n",
    "        dimensions = {'bscans': images_array.shape[0],\n",
    "                      'layers': len(layer_mapping.keys()),\n",
    "                      'height': images_array.shape[1],\n",
    "                      'width' : images_array.shape[2]\n",
    "        }\n",
    "        predicted_stack = get_predictions(images_array2, dimensions, relaynet_model)\n",
    "        predicted_stack_argmax = np.argmax(predicted_stack, axis=1)\n",
    "        if 'label_path' in list(filepaths.keys()):   \n",
    "            for scan in range(predicted_stack.shape[0]):\n",
    "                for layer in range(predicted_stack.shape[1]):\n",
    "                    mask = predicted_stack_argmax[scan]==layer\n",
    "                    mask=mask.astype(np.bool)\n",
    "                    gt = lmap_array[scan, layer]\n",
    "                    gt=gt.astype(np.bool)\n",
    "                    metric=evaluate(gt,mask)\n",
    "                    metric_layer_dict[layer_mapping[layer]].append(metric)\n",
    "\n",
    "                    if counter==0: # create csv file if not created before\n",
    "#                         os.makedirs(os.path.join(save_filepaths['evaluations_path'], save_filepaths['results_name']))\n",
    "                        with open(os.path.join(save_eval, save_filepaths['results_name']+\".csv\"),\"w\") as csvfile:\n",
    "                            filewriter = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL,lineterminator='\\n')\n",
    "                            filewriter.writerow([dataset,scan,layer_mapping[layer], str(metric)])\n",
    "                            counter=counter+1\n",
    "                    else: #otherwise, append to the csv file\n",
    "                        with open(os.path.join(save_eval, save_filepaths['results_name']+\".csv\"),\"a\") as csvfile:\n",
    "                            filewriter = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "                            filewriter.writerow([dataset,scan,layer_mapping[layer], str(metric)])\n",
    "            \n",
    "\n",
    "                cv2.imwrite(os.path.join(save_pred, 'Slice_1_'+str(scan+1)+'_argmax.png'), predicted_stack_argmax[scan])\n",
    "                plt.figure(figsize=(30,15))\n",
    "                plt.imshow(predicted_stack_argmax[scan], vmin=0, vmax=dimensions['layers']-1)\n",
    "                plt.savefig(os.path.join(save_pred_viz, 'Slice_1_'+str(scan+1)+'_argmax.png'))\n",
    "                plt.close()\n",
    "\n",
    "                np.save(os.path.join(save_filepaths['predictions_path'], dataset+'.npy'), predicted_stack_argmax)\n",
    "\n",
    "        else:\n",
    "            for scan in range(predicted_stack_argmax.shape[0]):\n",
    "                cv2.imwrite(os.path.join(save_pred, 'Slice_1_'+str(scan+1)+'_argmax.png'), predicted_stack_argmax[scan])\n",
    "                plt.figure(figsize=(30,15))\n",
    "                plt.imshow(predicted_stack_argmax[scan], vmin=0, vmax=dimensions['layers']-1)\n",
    "                plt.savefig(os.path.join(save_pred_viz, 'Slice_1_'+str(scan+1)+'_argmax.png'))\n",
    "                plt.close()\n",
    "            np.save(os.path.join(save_filepaths['predictions_path'], dataset+'.npy'), predicted_stack_argmax)\n",
    "            \n",
    "    if 'label_path' in list(filepaths.keys()): \n",
    "        statistics(metric_layer_dict,save_filepaths['evaluations_path'], save_filepaths['results_name'])\n",
    "        total_metric_df = pd.DataFrame(metric_layer_dict)\n",
    "        total_metric_df.to_csv(os.path.join(save_filepaths['evaluations_path'], save_filepaths['results_name']+'.csv'), index=False)\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
